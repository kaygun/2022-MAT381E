{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import gzip\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from xmltodict import parse\n",
    "from collections import Counter\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3316d03",
   "metadata": {},
   "source": [
    "# Open Data and Data APIs\n",
    "\n",
    "Large part of doing data science is working with data: cleaning, understanding, filtering, and tranforming it. But in order to do that we need data. Unless you collect your own data, you will need to find interesting data sets that you can understand and ask questions about. Today, we are going to look at possible data sources and their uses.\n",
    "\n",
    "An [application programming interface (API)](https://en.wikipedia.org/wiki/API) is a data connection between two pieces of software. For our purposes, it is a connection between a data consumer (you) and data provider.  Its primary function is **not** to provide data for human consumption, rather it is for exchanging data between two computer programs. In short, you'll use an API to fetch the data not to look at it in its raw form.\n",
    "\n",
    "\n",
    "## Toy Datasets for Teaching and Experimenting\n",
    "\n",
    "Sometimes, one needs data to learn a topic, make small experiments before embarking on *real* large datasets. For that there is one specific place: [UCI's data repository](https://archive.ics.uci.edu/ml/datasets.php).\n",
    "\n",
    "Here are a couple of examples:\n",
    "\n",
    "### The Wine Dataset\n",
    "\n",
    "\n",
    "The [wine data](https://archive.ics.uci.edu/ml/datasets/wine) consists of chemical analysis of wines produced in Italy from three different producers. Data has 14 columns: \n",
    "\n",
    "1. Alcohol\n",
    "2. Malic acid\n",
    "3. Ash\n",
    "4. Alcalinity of ash\n",
    "5. Magnesium\n",
    "6. Total phenols\n",
    "7. Flavanoids\n",
    "8. Nonflavanoid phenols\n",
    "9. Proanthocyanins\n",
    "10. Color intensity\n",
    "11. Hue\n",
    "12. OD280/OD315 of diluted wines\n",
    "13. Proline \n",
    "\n",
    "The remaining column is the label (0,1 and 2) indicating different producers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data') as url:\n",
    "    data = pd.read_csv(url, header=None)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[8],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53caa257",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c7420d",
   "metadata": {},
   "source": [
    "## Mammographic Masses\n",
    "\n",
    "[Data](https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/) is about mammography test results on breast tumors.  It is of columnar format (CSV) and contains 7 columns:\n",
    "\n",
    "1. BI-RADS assessment: 1 to 5 (ordinal, non-predictive!)\n",
    "2. Age: patient's age in years (integer)\n",
    "3. Shape: mass shape: round=1 oval=2 lobular=3 irregular=4 (nominal)\n",
    "4. Margin: mass margin: circumscribed=1 microlobulated=2 obscured=3 ill-defined=4 spiculated=5 (nominal)\n",
    "5. Density: mass density high=1 iso=2 low=3 fat-containing=4 (ordinal)\n",
    "\n",
    "The remaining column indicate whether the tumor is benign (encoded as 0) or malignant (encoded as 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79cb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data') as url:\n",
    "    data = pd.read_csv(url, header=None)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59661a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[1],8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b730146",
   "metadata": {},
   "source": [
    "## Bank Marketing Data Set\n",
    "\n",
    "[The data](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) is about marketing campaigns of a Portuguese bank. The marketing campaigns were done via phone. Data again is of columnar format. It contains 21 columns:\n",
    "\n",
    "1. age (numeric)\n",
    "2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4.  education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5.  default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6.  housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7.  loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "8.  contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9.  month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10.  day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11.  duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "12.  campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13.  pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14.  previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15.  poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "16.  emp.var.rate: employment variation rate.  quarterly indicator (numeric)\n",
    "17.  cons.price.idx: consumer price index.  monthly indicator (numeric)\n",
    "18.  cons.conf.idx: consumer confidence index.  monthly indicator (numeric)\n",
    "19.  euribor3m: euribor 3 month rate.  daily indicator (numeric)\n",
    "20.  nr.employed: number of employees.  quarterly indicator (numeric)\n",
    "\n",
    "\n",
    "The remaining column is indicates whether the customer bought the banking service that was advertised/marketed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511eefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip') as url:\n",
    "    zf = ZipFile(BytesIO(url.read()))\n",
    "    data = pd.read_csv(zf.open('bank.csv'),sep=';')\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e9430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(data['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6611f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabl = pd.crosstab(data['education'],data['default'])\n",
    "tabl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87730848",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    tabl.iloc[i,:] = tabl.iloc[i,:]/sum(tabl.iloc[i,:])\n",
    "tabl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ff00ea",
   "metadata": {},
   "source": [
    "## Data Science Competitions and Hackathons\n",
    "\n",
    "[Kaggle](https://kaggle.com) is a data science community page that serves those people who take data challenges as a competition. There are varieties of data science competitions that you can browse, try and even start working on to compete. However, the datasets are not open. This means, you must first register to kaggle before you can look into data and you can download data using their webpage or their specific client. The code below works only if you already are registered and put `kaggle.json` authentication key to your home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "api.dataset_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.competitions_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.dataset_download_files('kamilpytlak/personal-key-indicators-of-heart-disease')\n",
    "zip = ZipFile('personal-key-indicators-of-heart-disease.zip')\n",
    "zip.filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(zip.open('heart_2020_cleaned.csv'))\n",
    "zip.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.competition_download_files('titanic')\n",
    "zip = ZipFile('titanic.zip')\n",
    "zip.filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa20817",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(zip.open('train.csv'))\n",
    "zip.close()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = pd.crosstab(data['Survived'],data['Pclass'])\n",
    "\n",
    "for i in range(2):\n",
    "    surv.iloc[i,:] = surv.iloc[i,:]/surv.iloc[i,:].sum()\n",
    "\n",
    "surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf7c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = pd.crosstab(data['Survived'],data['Pclass'])\n",
    "\n",
    "for i in range(3):\n",
    "    surv.iloc[:,i] = surv.iloc[:,i]/surv.iloc[:,i].sum()\n",
    "    \n",
    "surv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff00d6",
   "metadata": {},
   "source": [
    "# Data From Municipalities\n",
    "\n",
    "We have used [data from Istanbul Municipality data service](https://data.ibb.gov.tr/). There are other municipalties that serves open data:\n",
    "\n",
    "1. [Istanbul Municipality](https://data.ibb.gov.tr/)\n",
    "2. [Izmir Municipality](https://acikveri.bizizmir.com/)\n",
    "3. [Bursa Municipality](https://acikyesil.bursa.bel.tr/dataset/)\n",
    "4. [Athens Open Data](http://geodata.gov.gr/en/dataset)\n",
    "4. [Barcelona Municipality](https://opendata-ajuntament.barcelona.cat/)\n",
    "5. [London Data Store](https://data.london.gov.uk/developers/)\n",
    "6. [New York Open Data](https://opendata.cityofnewyork.us/)\n",
    "7. [City of Montreal Open Data](https://donnees.montreal.ca/collections)\n",
    "8. [City of Toronto Open Data](https://open.toronto.ca/)\n",
    "\n",
    "Best way to explore is search \"open data api\" + your favorite city :) \n",
    "\n",
    "\n",
    "Here is the graph of mean natural gas consumption in Istanbul in (numbers are in 10 mil m3) per year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/kaygun/local/tmp/2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv') as file:\n",
    "    squirrel = pd.read_csv(file)\n",
    "    \n",
    "squirrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8180b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(squirrel['Tail twitches'],squirrel['Indifferent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac71de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with urlopen('https://data.ibb.gov.tr/dataset/02bdc2d6-94bb-4e31-816e-528bc9d98703/resource/d5fe41b0-3848-4548-9ac7-6e4756c3027b/download/ilce-baznda-yllara-gore-doalgaz-tuketim-miktar-tr-en.xlsx') as url:\n",
    "    data = pd.read_excel(url.read())\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.groupby('Yıl')['Dogalgaz Tüketim Miktarı (m3)'].mean()/1e+06).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c703bdd",
   "metadata": {},
   "source": [
    "# Data from Government Organizations\n",
    "\n",
    "1. [European Central Bank](https://sdw.ecb.europa.eu/)\n",
    "2. [OECD data](https://data.oecd.org/)\n",
    "3. [The US Central Bank (FED) data](https://fred.stlouisfed.org/)\n",
    "4. [The World Bank Data](https://data.worldbank.org/)\n",
    "5. [The US Goverment](https://data.gov/developers/apis/index.html) collected all of its open data sources under a single service.\n",
    "6. [Indian Government Data Portal](https://data.gov.in/)\n",
    "7. [European Union Data Portal](https://data.europa.eu/en)\n",
    "8. [Turkish Supreme Election Council](https://acikveri.ysk.gov.tr/anasayfa) (Yüksek Seçim Kurulu) also publishes critical data on all Turkish elections on their data service.\n",
    "9. [International Monetary Fund (IMF) Data Portal](https://www.imf.org/en/Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae825393",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/kaygun/local/tmp/WEOJanuary2022update.xlsx') as file:\n",
    "    IMF = pd.read_excel(file.read(),encoding='latin1')\n",
    "    \n",
    "IMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514485dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen(\"http://www.ecb.europa.eu/stats/eurofxref/eurofxref-daily.xml\") as conn:\n",
    "    data = parse(conn.read())['gesmes:Envelope']['Cube']['Cube']['Cube']\n",
    "\n",
    "rates = [ float(x['@rate']) for x in data ]\n",
    "dates = [ x['@currency'] for x in data ]\n",
    "\n",
    "df = pd.DataFrame({'rates': rates}, index=dates)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://api.worldbank.org/v2/en/indicator/SI.POV.DDAY?downloadformat=xml') as url:\n",
    "    zip = ZipFile(BytesIO(url.read()))\n",
    "    print(zip.filelist)\n",
    "    data = pd.read_xml(zip.open('API_SI.POV.DDAY_DS2_en_xml_v2_3732823.xml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a0ac2",
   "metadata": {},
   "source": [
    "# Geological Survey Data\n",
    "\n",
    "## US Geological Survey \n",
    "\n",
    "[USGS](https://www.usgs.gov/products) has a very large data store where you can get variety of scientific data that included Earthquakes, Satellite images, Maps, and much much more.\n",
    "\n",
    "## European Space Agency\n",
    "\n",
    "[European Space Agency](https://open.esa.int/) has an excellent open data service from which you can access a variety of data products such as maps, satellite images and more.\n",
    "\n",
    "## NASA \n",
    "\n",
    "[NASA](https://data.nasa.gov/) also has an open data service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6692580",
   "metadata": {},
   "source": [
    "### An Example from NASA\n",
    "\n",
    "####  Global Landslide Catalog Export\n",
    "\n",
    "> The Global Landslide Catalog (GLC) was developed with the goal of identifying rainfall-triggered landslide events > around the world, regardless of size, impacts or location. The GLC considers all types of mass movements triggered > by rainfall, which have been reported in the media, disaster databases, scientific reports, or other sources. The > GLC has been compiled since 2007 at NASA Goddard Space Flight Center. This is a unique data set with the ID tag “GLC” in the landslide editor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://data.nasa.gov/api/views/dd9e-wu2v/rows.csv?accessType=DOWNLOAD') as url:\n",
    "    data = pd.read_csv(url)\n",
    "    \n",
    "landslidesTR = data.loc[data['country_code'] == 'TR']\n",
    "landslidesTR[['event_date','admin_division_name','gazeteer_closest_point','longitude','latitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffaa8b",
   "metadata": {},
   "source": [
    "### An Example from USGS\n",
    "\n",
    "Paleohydrologic reconstructions of water-year streamflow for 31 stream gaging sites in the Missouri River Basin with complete data for 1685 through 1977"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0f2f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with urlopen('https://www.sciencebase.gov/catalog/file/get/5c994278e4b0b8a7f628903e?f=__disk__3d%2F8b%2F51%2F3d8b512c0dec73102f0c8d7b5c3e8d326dce54fa') as url:\n",
    "    data = pd.read_csv(url)\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114258ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.iloc[5,5:]).rolling(window=5).mean().plot(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0793cfd",
   "metadata": {},
   "source": [
    "# Image Classification Data Sets\n",
    "\n",
    "Here is a small sample of image dataset that can be used for image classification tasks:\n",
    "\n",
    "1. [MNIST](http://yann.lecun.com/exdb/mnist/) \n",
    "2. [Extended MNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset)\n",
    "3. [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
    "4. [Japanese MNIST](https://github.com/rois-codh/kmnist)\n",
    "5. [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "6. [Olivetti faces data set](https://scikit-learn.org/0.19/datasets/olivetti_faces.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz') as url:\n",
    "    fmnist = np.frombuffer(gzip.open(BytesIO(url.read()),'rb').read(), \n",
    "                           dtype=np.uint8,\n",
    "                           offset=16)\n",
    "\n",
    "fmnist = fmnist.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17faf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.random.randint(10000)\n",
    "plt.imshow(fmnist[N].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf57d22",
   "metadata": {},
   "source": [
    "# Satellite Image Data\n",
    "\n",
    "1. [Hyperspectral Remote Sensing Scenes](http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes)\n",
    "2. [A large list of open GIS Image data sets](https://freegisdata.rtwilson.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3523d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afe5a1ec",
   "metadata": {},
   "source": [
    "# APIs where you would have to register and login\n",
    "\n",
    "All of the data sources I quoted above are open. You don't neet to enter credentials to login and access the data. However, most commercial data vendors do ask you to register and login before you access their data. \n",
    "\n",
    "## NASDAQ Financial Data\n",
    "\n",
    "Here is an example: [NASDAQ](https://data.nasdaq.com/). Nasdaq is world's first electronic exchange platform for buying and trading securities. It has an extensive data collection on markets. But you would need their specific python library, and also register at their site (you'll need and API key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nasdaq-data-link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d958996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasdaqdatalink\n",
    "nasdaqdatalink.read_key(\"/home/kaygun/.config/apikey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d028f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nasdaqdatalink.get('CHRIS/CME_W1')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['High'].plot(figsize=(18,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322752d",
   "metadata": {},
   "source": [
    "## Twitter Data\n",
    "\n",
    "You can fetch data from [twitter](https://twitter.com) through their [API](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api) and analyze it via python libraries. [Tweepy](https://www.tweepy.org/) is a popular choice, but you should definitely look around and find one that suits your needs.\n",
    "\n",
    "Here is how I would install and import tweepy: Remember that \"!pip install tweepy\" should only be used once. Once you install it, you don't have to run that part ever again on your machine. But you must invoke \"import tweepy\" part every time you are going to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a9d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tweepy\n",
    "import tweepy as tw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a192e63",
   "metadata": {},
   "source": [
    "I put all the necessary login details for the api into a single JSON file in my own directory. When you register at twitter you should replace 'apikey' file with your own with the following structure:\n",
    "\n",
    "    {\"API_key\": \"your api key\",\n",
    "     \"API_secret_key\": \"your api secret key\",\n",
    "     \"access_token\": \"your access token\",\n",
    "     \"access_secret\": \"your access token secret\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/kaygun/.config/twitter/apikey') as file:\n",
    "    keys = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccff36",
   "metadata": {},
   "source": [
    "This is how you would login:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43069178",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuth1UserHandler(\n",
    "   keys[\"API_key\"], keys[\"API_secret_key\"], keys[\"access_token\"], keys[\"access_secret\"]\n",
    ")\n",
    "\n",
    "api = tw.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eee5cd",
   "metadata": {},
   "source": [
    "Let us collect some tweets from this month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.search_tweets(q='William Hurt', lang='en', count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752452a9",
   "metadata": {},
   "source": [
    "and display 10 of them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f0aa94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[x.text for x in tweets[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0350ded4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = api.search_users('Atabey_Kaygun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032dd350",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7790459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
